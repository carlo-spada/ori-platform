# Ori Platform - AI Engine Environment Variables
# Copy this file to .env and fill in your actual values

# ============================================================================
# Server Configuration
# ============================================================================
PORT=3002
ENVIRONMENT=development

# ============================================================================
# Model Configuration
# ============================================================================
# Sentence transformer model for embeddings
# Default: all-MiniLM-L6-v2 (fast, lightweight, no API key needed)
# Options: all-MiniLM-L6-v2, all-mpnet-base-v2, multi-qa-mpnet-base-dot-v1
EMBEDDING_MODEL=all-MiniLM-L6-v2

# Model will be downloaded on first run (~80MB for default model)
# Models are cached in ~/.cache/huggingface/

# ============================================================================
# API Integration
# ============================================================================
# Core API URL for making requests to backend
CORE_API_URL=http://localhost:3001

# Frontend URL for CORS configuration
FRONTEND_URL=http://localhost:3000

# ============================================================================
# CORS Configuration
# ============================================================================
# Additional allowed origins (optional, comma-separated)
# ALLOWED_ORIGINS=http://localhost:3000,http://localhost:3001

# ============================================================================
# Logging Configuration
# ============================================================================
# Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# Enable detailed request/response logging (useful for debugging)
# DEBUG_MODE=false

# ============================================================================
# Performance Configuration
# ============================================================================
# Maximum batch size for embedding generation
# MAX_BATCH_SIZE=32

# Number of worker processes (default: number of CPU cores)
# WORKERS=4

# Timeout for external API calls (seconds)
# API_TIMEOUT=30

# ============================================================================
# Cache Configuration (Optional)
# ============================================================================
# Enable caching for embeddings and responses
# ENABLE_CACHE=true

# Cache TTL in seconds (default: 1 hour)
# CACHE_TTL=3600

# ============================================================================
# Health Check Configuration
# ============================================================================
# Interval for health check updates (seconds)
# HEALTH_CHECK_INTERVAL=60

# ============================================================================
# Production Configuration (for Cloud Run deployment)
# ============================================================================
# When deploying to Google Cloud Run, these will be set automatically:
# - PORT: Will be set by Cloud Run
# - ENVIRONMENT: Set to 'production'
# - LOG_LEVEL: Recommended to set to 'WARNING' or 'ERROR'

# Cloud Run specific settings:
# - Memory: 2GB recommended
# - CPU: 2 vCPU recommended for production
# - Min instances: 0 (scale to zero when idle)
# - Max instances: 10 (adjust based on load)
# - Timeout: 300s (5 minutes)
